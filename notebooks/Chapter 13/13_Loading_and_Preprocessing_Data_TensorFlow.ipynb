{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e375b28",
   "metadata": {},
   "source": [
    "# **Chapter 13: Loading and Preprocessing Data with TensorFlow**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "Sejauh ini, kita sering bekerja dengan dataset yang cukup kecil untuk dimuat seluruhnya ke dalam memori (RAM). Namun, sistem Deep Learning modern sering kali dilatih menggunakan dataset yang sangat besar yang tidak muat dalam RAM. Mengelola dataset besar dan melakukan preprocessing secara efisien bisa menjadi hal yang rumit, tetapi TensorFlow menyederhanakannya melalui **Data API**.\n",
    "\n",
    "Data API memungkinkan kita untuk membuat objek dataset, memberi tahu dari mana data diambil, dan bagaimana cara mentransformasikannya. TensorFlow menangani detail implementasi seperti *multithreading*, *queuing*, *batching*, dan *prefetching* secara otomatis. Selain itu, Data API bekerja mulus dengan `tf.keras`.\n",
    "\n",
    "Dalam bab ini, kita akan membahas:\n",
    "* **Data API**: Membangun *input pipeline* yang efisien.\n",
    "* **TFRecord Format**: Format biner fleksibel dan efisien untuk data besar.\n",
    "* **Preprocessing**: Menggunakan layer preprocessing Keras standar dan kustom (seperti encoding fitur kategorikal dan *embeddings*).\n",
    "* **TensorFlow Datasets (TFDS)**: Cara mudah memuat dataset standar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278f730",
   "metadata": {},
   "source": [
    "## **2. The Data API**\n",
    "\n",
    "Konsep inti dari Data API adalah `dataset`, yang merepresentasikan urutan item data. Kita bisa membaca data dari disk, namun untuk mempermudah pemahaman awal, mari kita buat dataset dari memori menggunakan `tf.data.Dataset.from_tensor_slices()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Membuat dataset dari tensor sederhana (angka 0 sampai 9)\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Iterasi item dalam dataset\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd155932",
   "metadata": {},
   "source": [
    "### **Rantai Transformasi (Chaining Transformations)**\n",
    "\n",
    "Setelah memiliki objek dataset, kita dapat menerapkan berbagai transformasi. Setiap metode transformasi mengembalikan dataset baru, sehingga kita bisa merangkainya (*chaining*).\n",
    "\n",
    "Operasi umum meliputi:\n",
    "* `repeat(n)`: Mengulangi dataset sebanyak `n` kali.\n",
    "* `batch(n)`: Mengelompokkan item ke dalam batch berukuran `n`.\n",
    "* `map(fn)`: Menerapkan fungsi transformasi pada setiap item.\n",
    "* `shuffle(buffer_size)`: Mengacak data untuk memastikan independensi instance training.\n",
    "* `filter(fn)`: Menyaring data berdasarkan kondisi tertentu.\n",
    "* `take(n)`: Mengambil `n` item pertama saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh rantai transformasi: Repeat, Batch, dan Map\n",
    "# 1. Ulangi dataset 3 kali\n",
    "# 2. Kelompokkan dalam batch berisi 7 item\n",
    "# 3. Gandakan nilai setiap item (map)\n",
    "\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cd305",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "Metode `batch(7)` akan menghasilkan tensor berukuran 7. Jika sisa data di akhir kurang dari 7, batch terakhir akan lebih kecil (kecuali jika `drop_remainder=True` digunakan).\n",
    "\n",
    "### **Mengacak Data (Shuffling)**\n",
    "\n",
    "Gradient Descent bekerja paling baik jika data training bersifat *independent and identically distributed* (IID). Metode `shuffle()` menggunakan buffer untuk mengacak data.\n",
    "\n",
    "Penting untuk menentukan `buffer_size` yang cukup besar agar pengacakan efektif, tetapi tidak melebihi kapasitas RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengacak dataset dengan buffer size 5 dan random seed 42\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a639a08",
   "metadata": {},
   "source": [
    "### **Interleaving: Membaca dari Banyak File**\n",
    "\n",
    "Untuk dataset skala besar, data biasanya dipecah ke dalam banyak file. Kita dapat membaca file-file ini secara acak dan menyisipkan (*interleave*) baris-barisnya untuk pengacakan yang lebih baik.\n",
    "\n",
    "Fungsi `interleave()` memungkinkan kita membaca dari beberapa file sekaligus secara paralel. Berikut adalah contoh fungsi `csv_reader_dataset` yang menggabungkan pembacaan file CSV, preprocessing, shuffling, dan batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    # 1. List file paths dan acak urutannya\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    \n",
    "    # 2. Interleave: Membaca dari n_readers file sekaligus\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skip header\n",
    "        cycle_length=n_readers, \n",
    "        num_parallel_calls=n_read_threads)\n",
    "    \n",
    "    # 3. Preprocessing (map) dan Shuffling\n",
    "    # Asumsi fungsi 'preprocess' sudah didefinisikan sebelumnya\n",
    "    # dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    \n",
    "    # 4. Batching dan Prefetching\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910eb68e",
   "metadata": {},
   "source": [
    "### **Prefetching: Meningkatkan Performa**\n",
    "\n",
    "Metode `prefetch(1)` di akhir pipeline sangat krusial untuk performa. Ini membuat dataset selalu menyiapkan satu batch di depan sementara GPU sedang melatih batch saat ini.\n",
    "\n",
    "\n",
    "\n",
    "Dengan *prefetching*, CPU dan GPU bekerja secara paralel, sehingga GPU tidak perlu menunggu data dimuat, yang secara dramatis mempercepat waktu pelatihan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee93bf",
   "metadata": {},
   "source": [
    "## **3. Format TFRecord**\n",
    "\n",
    "TFRecord adalah format pilihan TensorFlow untuk menyimpan data dalam jumlah besar. Ini adalah format biner sederhana yang berisi urutan *binary records*. Format ini sangat efisien untuk dibaca secara sekuensial.\n",
    "\n",
    "### **Menulis dan Membaca TFRecord**\n",
    "Kita bisa menggunakan `tf.io.TFRecordWriter` untuk menulis dan `tf.data.TFRecordDataset` untuk membaca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menulis ke file TFRecord\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "# Membaca dari file TFRecord\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e40d0fd",
   "metadata": {},
   "source": [
    "### **Protocol Buffers (Protobufs)**\n",
    "\n",
    "Meskipun TFRecord bisa menyimpan string biner apa saja, biasanya ia digunakan untuk menyimpan **Protocol Buffers** (protobuf) yang diserialisasi. Protobuf yang paling umum digunakan adalah `Example`.\n",
    "\n",
    "Protobuf `Example` merepresentasikan satu instance dalam dataset dan berisi daftar fitur bernama. Berikut cara membuat dan menulis `Example` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25149f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "# Membuat objek Example Protobuf\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }))\n",
    "\n",
    "# Serialisasi dan tulis ke file\n",
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dffb6",
   "metadata": {},
   "source": [
    "Untuk membaca kembali data tersebut, kita perlu mendefinisikan deskripsi fitur dan menggunakan `tf.io.parse_single_example`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219483f",
   "metadata": {},
   "source": [
    "## **4. Preprocessing Fitur Input**\n",
    "\n",
    "Data mentah sering kali perlu dikonversi menjadi numerik, dinormalisasi, atau di-*encode* sebelum masuk ke jaringan saraf.\n",
    "\n",
    "### **Encoding Fitur Kategorikal (One-Hot)**\n",
    "Jika kita memiliki fitur kategorikal dengan sedikit kategori (misal: \"NEAR BAY\", \"INLAND\"), kita bisa menggunakan *one-hot encoding*. Kita perlu memetakan setiap kategori ke indeks menggunakan *lookup table*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc87b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Lookup Table untuk kategori\n",
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "\n",
    "# Inisialisasi Lookup Table dengan OOV (Out-of-Vocabulary) buckets\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "\n",
    "# Menguji lookup dan one-hot encoding\n",
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "\n",
    "print(\"Indices:\", cat_indices)\n",
    "print(\"One-hot:\\n\", cat_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394c749",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "Kategori yang tidak dikenal (seperti \"DESERT\") akan dipetakan ke salah satu *oov buckets* menggunakan *hashing*, untuk menghindari hilangnya informasi sepenuhnya.\n",
    "\n",
    "### **Embeddings**\n",
    "\n",
    "Untuk fitur kategorikal dengan banyak kategori (misal: 50+), *one-hot encoding* menjadi tidak efisien. Solusinya adalah menggunakan **Embeddings**.\n",
    "\n",
    "Embedding adalah vektor padat (*dense vector*) yang dapat dilatih, yang merepresentasikan sebuah kategori. Awalnya diinisialisasi secara acak, namun selama pelatihan, *Gradient Descent* akan mendorong kategori yang mirip agar memiliki representasi vektor yang dekat di ruang embedding.\n",
    "\n",
    "\n",
    "\n",
    "Keras menyediakan layer `keras.layers.Embedding` untuk menangani ini secara otomatis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7648284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Layer Embedding di Keras\n",
    "embedding_dim = 2\n",
    "embed_layer = tf.keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, \n",
    "                                        output_dim=embedding_dim)\n",
    "\n",
    "# Mengambil vektor embedding untuk indeks kategori\n",
    "embeddings = embed_layer(cat_indices)\n",
    "print(\"Embeddings:\\n\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df052e40",
   "metadata": {},
   "source": [
    "### **Keras Preprocessing Layers**\n",
    "\n",
    "Keras kini menyediakan layer standar untuk preprocessing yang bisa dimasukkan langsung ke dalam model, seperti:\n",
    "* `Normalization`: Untuk standarisasi fitur (rata-rata 0, deviasi standar 1).\n",
    "* `TextVectorization`: Untuk mengubah teks menjadi urutan token atau vektor hitungan kata (Bag-of-Words/TF-IDF).\n",
    "* `Discretization`: Untuk memotong data kontinu menjadi *bin*.\n",
    "\n",
    "Layer ini memiliki metode `adapt()` yang memungkinkan layer mempelajari statistik data (seperti *vocabulary* atau *mean/std*) sebelum pelatihan dimulai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f111b2",
   "metadata": {},
   "source": [
    "## **5. TensorFlow Datasets (TFDS)**\n",
    "\n",
    "Proyek TensorFlow Datasets (TFDS) memudahkan kita mengunduh dan memuat dataset standar seperti MNIST, ImageNet, atau dataset teks. TFDS menyediakan dataset yang sudah siap digunakan dengan Data API.\n",
    "\n",
    "Berikut contoh memuat dataset MNIST menggunakan TFDS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8da18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Memuat dataset MNIST (as_supervised=True memberikan tuple (fitur, label))\n",
    "dataset, info = tfds.load(name=\"mnist\", as_supervised=True, with_info=True)\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Menerapkan transformasi pipeline\n",
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)\n",
    "\n",
    "print(\"Dataset MNIST siap digunakan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044700c3",
   "metadata": {},
   "source": [
    "## **6. Kesimpulan**\n",
    "\n",
    "Bab ini mengajarkan kita bahwa memuat dan memproses data secara efisien adalah kunci dalam Deep Learning skala besar. Kita telah mempelajari:\n",
    "* **Data API** memungkinkan pembuatan pipeline yang kompleks dengan *chaining* metode seperti `map`, `shuffle`, dan `batch`.\n",
    "* **Prefetching** dan **Interleaving** memaksimalkan penggunaan GPU dengan memparalelkan proses I/O CPU.\n",
    "* **TFRecord** adalah format yang disarankan untuk penyimpanan data besar yang efisien menggunakan Protocol Buffers.\n",
    "* **Preprocessing** seperti *One-Hot Encoding* dan *Embeddings* sangat penting untuk menangani data kategorikal dan teks agar dapat diproses oleh Neural Network.\n",
    "\n",
    "Dengan alat-alat ini, kita siap menangani dataset yang jauh lebih besar dari kapasitas memori komputer kita."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
