{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6663ef09",
   "metadata": {},
   "source": [
    "# **Chapter 8: Dimensionality Reduction**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "[cite_start]Banyak masalah Machine Learning di dunia nyata melibatkan ribuan bahkan jutaan fitur untuk setiap instance pelatihan[cite: 631]. [cite_start]Banyaknya fitur ini tidak hanya membuat proses pelatihan menjadi sangat lambat, tetapi juga menyulitkan algoritma untuk menemukan solusi yang optimal[cite: 632]. [cite_start]Fenomena ini dikenal sebagai **Curse of Dimensionality** (Kutukan Dimensi)[cite: 633].\n",
    "\n",
    "**Mengapa Bab ini Penting?**\n",
    "[cite_start]Dalam banyak kasus praktis, kita dapat mengurangi jumlah fitur secara signifikan tanpa kehilangan banyak informasi[cite: 634]. [cite_start]Pengurangan dimensi membantu mempercepat pelatihan dan sangat berguna untuk visualisasi data (DataViz)[cite: 642, 643]. [cite_start]Dengan mengurangi dimensi menjadi dua atau tiga, kita dapat melihat pola seperti cluster pada grafik[cite: 644, 646].\n",
    "\n",
    "Dalam bab ini, kita akan membahas:\n",
    "* **The Curse of Dimensionality**\n",
    "* **Pendekatan Utama:** Proyeksi dan Manifold Learning\n",
    "* **Teknik Populer:** PCA, Kernel PCA, dan LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c81d1",
   "metadata": {},
   "source": [
    "## **2. The Curse of Dimensionality**\n",
    "\n",
    "[cite_start]Intuisi kita sering kali gagal dalam membayangkan ruang berdimensi tinggi[cite: 651]. [cite_start]Sebagai contoh, dalam hypercube 10.000 dimensi, lebih dari 99,99% titik berada sangat dekat dengan perbatasan[cite: 664, 665].\n",
    "\n",
    "### **Sparsity dan Overfitting**\n",
    "* [cite_start]Dataset berdimensi tinggi berisiko menjadi sangat jarang (*sparse*), di mana sebagian besar instance pelatihan kemungkinan besar saling berjauhan[cite: 676].\n",
    "* [cite_start]Hal ini membuat prediksi menjadi kurang andal karena didasarkan pada ekstrapolasi yang jauh lebih besar dibandingkan dimensi rendah[cite: 677].\n",
    "* [cite_start]Semakin banyak dimensi, semakin besar risiko model mengalami *overfitting* pada data pelatihan[cite: 678].\n",
    "* [cite_start]Secara teoritis, untuk menjaga kepadatan data yang cukup, jumlah instance yang dibutuhkan tumbuh secara eksponensial terhadap jumlah dimensi[cite: 680]. [cite_start]Bahkan dengan 100 fitur, kita butuh lebih banyak instance daripada atom di alam semesta agar data tidak tersebar terlalu jarang[cite: 681].\n",
    "\n",
    "[cite_start]Konsep dimensi ini diilustrasikan secara konseptual pada Gambar 8-1 di buku yang menunjukkan perkembangan dari titik (0D) hingga tesseract (4D)[cite: 661]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d54871",
   "metadata": {},
   "source": [
    "## **3. Pendekatan Utama: Proyeksi vs Manifold Learning**\n",
    "\n",
    "### **Proyeksi (Projection)**\n",
    "[cite_start]Dalam banyak masalah nyata, instance pelatihan tidak tersebar seragam, melainkan berada di dalam atau dekat dengan *subspace* berdimensi lebih rendah[cite: 685, 687]. [cite_start]Teknik proyeksi bekerja dengan memproyeksikan instance secara tegak lurus ke *subspace* tersebut[cite: 711]. [cite_start]Hal ini divisualisasikan dalam Gambar 8-2 dan 8-3 di buku, di mana data 3D diproyeksikan menjadi 2D[cite: 709, 720].\n",
    "\n",
    "### **Manifold Learning**\n",
    "[cite_start]Proyeksi tidak selalu ideal jika *subspace* terpelintir, seperti pada dataset **Swiss Roll** (Gambar 8-4)[cite: 723]. [cite_start]Memproyeksikan Swiss Roll begitu saja akan menumpuk lapisan-lapisan data[cite: 738]. [cite_start]Manifold Learning berasumsi bahwa dataset terletak dekat dengan manifold berdimensi rendah yang bisa \"dibuka gulungannya\" (Gambar 8-5)[cite: 739, 769]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa9b91",
   "metadata": {},
   "source": [
    "## **4. PCA (Principal Component Analysis)**\n",
    "\n",
    "[cite_start]PCA adalah algoritma pengurangan dimensi yang paling populer[cite: 834]. [cite_start]Tujuannya adalah mengidentifikasi *hyperplane* yang paling dekat dengan data dan memproyeksikannya ke sana[cite: 835].\n",
    "\n",
    "### **Menjaga Varians**\n",
    "[cite_start]PCA memilih sumbu yang menjaga jumlah varians maksimum, karena kemungkinan besar akan kehilangan lebih sedikit informasi (Gambar 8-7)[cite: 840, 858]. [cite_start]Sumbu ini juga meminimalkan jarak rata-rata kuadrat antara dataset asli dan proyeksinya[cite: 859].\n",
    "\n",
    "### **Principal Components**\n",
    "[cite_start]PCA menemukan sumbu yang menyumbang varians terbesar (PC pertama), lalu sumbu kedua yang ortogonal terhadap yang pertama untuk sisa varians, dan seterusnya[cite: 862, 863, 865]. [cite_start]Sumbu ke-i disebut *Principal Component* (PC) ke-i[cite: 866].\n",
    "\n",
    "### **Implementasi Manual dengan SVD**\n",
    "[cite_start]Teknik standar untuk menemukan PC adalah *Singular Value Decomposition* (SVD) yang mendekomposisi matriks X menjadi $U \\Sigma V^T$[cite: 877]. [cite_start]Penting untuk diingat bahwa data harus dipusatkan (*centered*) sebelum menjalankan PCA[cite: 886]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6deb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Membuat dataset 3D sederhana\n",
    "np.random.seed(42)\n",
    "m = 60\n",
    "X = np.random.randn(m, 3)\n",
    "\n",
    "# 1. Memusatkan data (Centering)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# 2. Menjalankan SVD\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "# 3. Ekstrak dua PC pertama\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "\n",
    "print(\"Principal Component 1:\", c1)\n",
    "print(\"Principal Component 2:\", c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6a539",
   "metadata": {},
   "source": [
    "### **Menggunakan Scikit-Learn**\n",
    "[cite_start]Kelas `PCA` di Scikit-Learn secara otomatis menangani pemusatan data dan menggunakan dekomposisi SVD[cite: 903, 904]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "# Melihat rasio varians yang dijelaskan\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe9716",
   "metadata": {},
   "source": [
    "**Interpretasi Hasil:**\n",
    "[cite_start]Atribut `explained_variance_ratio_` menunjukkan proporsi varians dataset yang terletak di sepanjang setiap komponen utama[cite: 910, 911]. [cite_start]Jika rasio PC pertama tinggi (misal 84%), berarti sebagian besar informasi asli terjaga pada sumbu tersebut[cite: 913, 915]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c3aa5",
   "metadata": {},
   "source": [
    "## **5. Memilih Jumlah Dimensi dan Kompresi**\n",
    "\n",
    "### **Memilih Dimensi yang Tepat**\n",
    "[cite_start]Alih-alih memilih angka sembarangan, lebih baik memilih jumlah dimensi yang mencakup porsi varians yang cukup besar, misalnya 95%[cite: 918]. [cite_start]Kita bisa memplot varians yang dijelaskan sebagai fungsi jumlah dimensi dan mencari \"sikut\" (*elbow*) pada kurva (Gambar 8-8)[cite: 928, 929].\n",
    "\n",
    "### **PCA untuk Kompresi**\n",
    "[cite_start]Setelah pengurangan dimensi, dataset memakan ruang yang jauh lebih sedikit[cite: 944]. [cite_start]Misalnya, pada MNIST, mempertahankan 95% varians mengurangi fitur dari 784 menjadi sekitar 154 fitur saja[cite: 945, 946]. [cite_start]Kita dapat melakukan rekonstruksi kembali ke dimensi asli menggunakan transformasi invers, meski akan ada sedikit kehilangan informasi yang disebut *reconstruction error*[cite: 949, 951]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5de0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh menemukan jumlah komponen untuk 95% varians\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print(\"Jumlah komponen terpilih:\", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35f2cc",
   "metadata": {},
   "source": [
    "## **6. Variasi PCA**\n",
    "\n",
    "* **Randomized PCA:** [cite_start]Algoritma stokastik yang lebih cepat untuk menemukan perkiraan komponen utama jika jumlah dimensi target (d) jauh lebih kecil dari jumlah fitur asli (n)[cite: 976, 977].\n",
    "* **Incremental PCA (IPCA):** [cite_start]Memungkinkan pelatihan dalam mini-batch, sangat berguna jika dataset terlalu besar untuk masuk ke memori (RAM)[cite: 981, 983].\n",
    "* **Kernel PCA (kPCA):** [cite_start]Menggunakan *kernel trick* untuk melakukan proyeksi non-linear yang kompleks[cite: 1004, 1005]. [cite_start]Hasil kPCA dengan berbagai kernel (Linear, RBF, Sigmoid) ditunjukkan pada Gambar 8-10 di buku[cite: 1015, 1042]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb69d2",
   "metadata": {},
   "source": [
    "## **7. LLE (Locally Linear Embedding)**\n",
    "\n",
    "[cite_start]LLE adalah teknik Manifold Learning non-linear yang tidak mengandalkan proyeksi[cite: 1104, 1105].\n",
    "\n",
    "### **Cara Kerja LLE**\n",
    "1. [cite_start]Mengidentifikasi k tetangga terdekat untuk setiap instance[cite: 1133].\n",
    "2. [cite_start]Mencoba merekonstruksi setiap instance sebagai fungsi linear dari tetangga-tetangganya (mencari bobot optimal)[cite: 1133, 1137].\n",
    "3. [cite_start]Memetakan instance ke ruang dimensi rendah sambil menjaga hubungan lokal tersebut semaksimal mungkin[cite: 1142, 1149].\n",
    "\n",
    "[cite_start]LLE sangat baik dalam \"membuka gulungan\" manifold yang terpelintir (Gambar 8-12)[cite: 1107, 1131]. [cite_start]Namun, LLE tidak berskala dengan baik untuk dataset yang sangat besar karena kompleksitas komputasinya yang melibatkan $m^2$[cite: 1156]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X_sr, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_reduced_lle = lle.fit_transform(X_sr)\n",
    "\n",
    "print(\"Bentuk data setelah LLE:\", X_reduced_lle.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e4d54",
   "metadata": {},
   "source": [
    "## **8. Teknik Lain dan Kesimpulan**\n",
    "\n",
    "[cite_start]Beberapa teknik populer lainnya (Gambar 8-13) meliputi[cite: 1176]:\n",
    "* **MDS:** [cite_start]Menjaga jarak antar instance[cite: 1166].\n",
    "* **Isomap:** [cite_start]Menjaga jarak *geodesic* (jarak terpendek melalui manifold) antar instance[cite: 1169].\n",
    "* **t-SNE:** [cite_start]Menjaga instance yang mirip tetap dekat dan yang berbeda tetap jauh; sangat populer untuk visualisasi cluster[cite: 1171, 1172].\n",
    "* **LDA:** [cite_start]Algoritma klasifikasi yang mempelajari sumbu paling diskriminatif antar kelas untuk proyeksi[cite: 1173, 1174].\n",
    "\n",
    "### **Ringkasan**\n",
    "[cite_start]Dimensionality Reduction membantu mempercepat pelatihan dan mempermudah visualisasi data kompleks[cite: 642, 643]. [cite_start]Meskipun akan ada sedikit kehilangan informasi dan penambahan kompleksitas pada pipeline, teknik seperti PCA dan LLE sangat krusial dalam kotak peralatan ilmuwan data[cite: 638, 639].\n",
    "\n",
    "Bab ini telah membekali kita dengan pemahaman mendalam tentang cara menangani data berdimensi tinggi sebelum melanjutkan ke topik-topik berikutnya dalam Machine Learning."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
