{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4fe7ab",
   "metadata": {},
   "source": [
    "# **Chapter 18: Reinforcement Learning**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "Reinforcement Learning (RL) adalah salah satu bidang paling menarik dalam Machine Learning. Berbeda dengan *Supervised Learning* (belajar dari jawaban yang benar) atau *Unsupervised Learning* (mencari pola dalam data tanpa label), RL adalah tentang belajar melalui **trial and error** (coba-coba).\n",
    "\n",
    "**Konsep Inti:**\n",
    "* **Agent:** Entitas yang belajar dan membuat keputusan (misal: robot atau karakter game).\n",
    "* **Environment:** Dunia tempat agen beroperasi (misal: papan permainan atau simulasi fisika).\n",
    "* **Action:** Tindakan yang dilakukan agen.\n",
    "* **Reward:** Umpan balik dari lingkungan (positif atau negatif) berdasarkan tindakan agen.\n",
    "* **Observation:** Informasi yang diterima agen tentang status lingkungan saat ini.\n",
    "\n",
    "Tujuan agen adalah memaksimalkan total *reward* yang dikumpulkan seiring waktu. Contoh sukses yang terkenal adalah AlphaGo dari DeepMind yang mengalahkan juara dunia Go.\n",
    "\n",
    "Dalam bab ini, kita akan mempelajari dasar-dasar RL, mulai dari kebijakan sederhana hingga Deep Q-Learning (DQN) yang mampu memainkan game Atari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c639d7e",
   "metadata": {},
   "source": [
    "## **2. Menyiapkan Lingkungan dengan OpenAI Gym**\n",
    "\n",
    "Salah satu tantangan dalam RL adalah memiliki lingkungan simulasi yang standar untuk melatih agen. OpenAI Gym (sekarang sering disebut Gymnasium) adalah toolkit standar untuk ini.\n",
    "\n",
    "Kita akan menggunakan lingkungan klasik **CartPole**. Tujuannya adalah menyeimbangkan tongkat di atas kereta yang bergerak dengan menggerakkan kereta ke kiri atau ke kanan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Membuat lingkungan CartPole\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Inisialisasi lingkungan\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Contoh observasi:\", obs)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# Visualisasi satu frame\n",
    "plt.imshow(env.render())\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Contoh mengambil satu tindakan acak\n",
    "action = 1 # 1 = Gerak Kanan, 0 = Gerak Kiri\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(f\"Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a837636",
   "metadata": {},
   "source": [
    "**Interpretasi:**\n",
    "* **Observation:** Array berisi 4 angka: posisi kereta, kecepatan kereta, sudut tongkat, dan kecepatan sudut tongkat.\n",
    "* **Action:** Diskrit (0 atau 1).\n",
    "* **Reward:** +1 untuk setiap langkah waktu tongkat tetap tegak.\n",
    "* **Done:** True jika tongkat jatuh atau kereta keluar dari layar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1d4b6",
   "metadata": {},
   "source": [
    "## **3. Kebijakan (Policy)**\n",
    "\n",
    "**Policy** adalah algoritma yang digunakan agen untuk menentukan tindakan berdasarkan observasi. Secara matematis ditulis sebagai $\\pi(s) = a$.\n",
    "\n",
    "### **Hardcoded Policy**\n",
    "Sebagai permulaan, mari buat kebijakan sederhana: \"Jika tongkat miring ke kiri, dorong kereta ke kiri. Jika miring ke kanan, dorong ke kanan.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41514ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(50):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "print(f\"Rata-rata reward (Mean): {np.mean(totals)}\")\n",
    "print(f\"Standar Deviasi (Std): {np.std(totals)}\")\n",
    "print(f\"Min: {np.min(totals)}, Max: {np.max(totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c82a7",
   "metadata": {},
   "source": [
    "**Analisis:**\n",
    "Kebijakan sederhana ini mungkin menjaga tongkat tetap tegak sebentar, tetapi tidak stabil. Untuk hasil yang lebih baik, kita memerlukan **Neural Network Policy** yang bisa belajar sendiri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934525e",
   "metadata": {},
   "source": [
    "## **4. Neural Network Policy**\n",
    "\n",
    "Kita bisa menggunakan Neural Network untuk menerima observasi sebagai input dan mengeluarkan probabilitas untuk setiap tindakan.\n",
    "\n",
    "### **The Credit Assignment Problem**\n",
    "Jika agen mendapatkan reward +100 setelah 50 langkah, tindakan mana yang bertanggung jawab atas kesuksesan itu? Apakah tindakan terakhir? Atau tindakan 10 langkah lalu?\n",
    "\n",
    "Ini disebut **Credit Assignment Problem**. Solusi standarnya adalah menggunakan **Discount Factor** ($\\gamma$). Reward di masa depan didiskon pada setiap langkah.\n",
    "* Reward total ($G_t$) = $R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\dots$\n",
    "* Jika $\\gamma$ mendekati 0, agen rabun jauh (mementingkan reward instan).\n",
    "* Jika $\\gamma$ mendekati 1, agen memiliki pandangan jauh ke depan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_inputs = 4\n",
    "n_outputs = 2\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(n_outputs, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "def play_one_step(env, obs, model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probas = model(obs[np.newaxis])\n",
    "        logits = tf.math.log(probas + keras.backend.epsilon())\n",
    "        action = tf.random.categorical(logits, num_samples=1)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=action[0], logits=logits\n",
    "        )\n",
    "    obs, reward, done, truncated, info = env.step(action[0, 0].numpy())\n",
    "    return obs, reward, done, truncated, loss, tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f390293",
   "metadata": {},
   "source": [
    "## **5. Policy Gradients (REINFORCE Algorithm)**\n",
    "\n",
    "Algoritma REINFORCE bekerja dengan cara:\n",
    "1. Biarkan agen bermain beberapa episode.\n",
    "2. Hitung total *return* (reward terdiskon) untuk setiap episode.\n",
    "3. Jika reward tinggi, buat tindakan yang diambil menjadi lebih mungkin terjadi di masa depan.\n",
    "4. Jika reward rendah, buat tindakan tersebut kurang mungkin terjadi.\n",
    "\n",
    "Secara teknis, kita menghitung gradien dari loss function, tetapi tidak langsung menerapkannya. Kita mengalikannya dengan *return* sebagai bobot, lalu menghitung rata-ratanya sebelum melakukan update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [\n",
    "        discount_rewards(rewards, discount_factor)\n",
    "        for rewards in all_rewards\n",
    "    ]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [\n",
    "        (discounted_rewards - reward_mean) / reward_std\n",
    "        for discounted_rewards in all_discounted_rewards\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a852652",
   "metadata": {},
   "source": [
    "## **6. Markov Decision Processes (MDP) & Q-Learning**\n",
    "\n",
    "$Q(s, a)$ adalah nilai yang diharapkan dari total discounted reward jika agen berada di status $s$, mengambil aksi $a$, dan bertindak optimal setelahnya.\n",
    "\n",
    "$$Q^*(s, a) = R(s, a) + \\gamma \\max_{a'} Q^*(s', a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf555a5",
   "metadata": {},
   "source": [
    "## **7. Deep Q-Learning (DQN)**\n",
    "\n",
    "DQN menggunakan Neural Network untuk mendekati Q-Function dengan Experience Replay dan Target Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "print(\"Komponen DQN siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29503643",
   "metadata": {},
   "source": [
    "## **8. Kesimpulan**\n",
    "\n",
    "Kita telah mempelajari dasar Reinforcement Learning, Policy Gradients, dan Deep Q-Learning."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
