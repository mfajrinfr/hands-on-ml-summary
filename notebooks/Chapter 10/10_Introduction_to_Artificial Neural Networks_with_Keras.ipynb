{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b92523",
   "metadata": {},
   "source": [
    "# **Bab 10: Pengantar Jaringan Saraf Tiruan dengan Keras**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "[cite_start]Jaringan Saraf Tiruan (Artificial Neural Networks atau ANNs) adalah model Machine Learning yang terinspirasi oleh struktur jaringan neuron biologis di otak kita[cite: 5]. [cite_start]Meskipun awalnya didasarkan pada analogi biologis, seiring waktu ANNs berkembang menjadi sistem yang berbeda dari aslinya, namun tetap sangat kuat dan fleksibel[cite: 7, 8]. \n",
    "\n",
    "ANNs merupakan inti dari Deep Learning. [cite_start]Kemampuannya yang dapat diskalakan (scalable) menjadikannya ideal untuk menangani tugas-tugas kompleks, seperti klasifikasi miliaran gambar, layanan pengenalan suara, hingga sistem rekomendasi video yang digunakan oleh jutaan orang setiap harinya[cite: 9]. [cite_start]Dalam notebook ini, kita akan mempelajari sejarah singkat ANNs, konsep dasarnya, dan bagaimana mengimplementasikannya menggunakan API Keras yang populer[cite: 10, 11]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c01bd",
   "metadata": {},
   "source": [
    "## **2. Konsep Dasar: Dari Neuron Biologis ke Tiruan**\n",
    "\n",
    "### **Neuron Biologis**\n",
    "[cite_start]Secara intuitif, neuron biologis adalah sel khusus yang terdiri dari tubuh sel, dendrit (cabang penerima sinyal), dan akson (cabang pengirim sinyal)[cite: 48]. [cite_start]Ketika neuron menerima sinyal kimiawi (neurotransmitter) yang cukup, ia akan \"menembakkan\" impuls listrik ke neuron lain[cite: 51, 52].\n",
    "\n",
    "### **Neuron Tiruan: Threshold Logic Unit (TLU)**\n",
    "[cite_start]Model komputasi paling sederhana dari neuron biologis adalah *Threshold Logic Unit* (TLU)[cite: 111]. Alih-alih sinyal kimia, TLU bekerja dengan angka:\n",
    "* **Input ($x$):** Fitur yang masuk.\n",
    "* **Bobot ($w$):** Menentukan seberapa penting setiap input.\n",
    "* [cite_start]**Jumlah Berbobot ($z$):** Menghitung total input dikalikan bobotnya ($z = x^T w$)[cite: 113].\n",
    "* [cite_start]**Fungsi Aktivasi (Step Function):** Jika hasil penjumlahan melebihi ambang batas tertentu, ia mengeluarkan output (biasanya 1 atau 0)[cite: 113, 115].\n",
    "\n",
    "### **Perceptron**\n",
    "[cite_start]Perceptron adalah salah satu arsitektur ANN paling awal, yang terdiri dari satu lapisan TLU[cite: 110, 140]. [cite_start]Semua neuron input terhubung ke setiap neuron di lapisan output, yang disebut sebagai *dense layer* atau lapisan yang terhubung penuh[cite: 141]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b7893",
   "metadata": {},
   "source": [
    "### **Reproduksi Kode: Perceptron dengan Scikit-Learn**\n",
    "\n",
    "Kode berikut menunjukkan cara menggunakan model Perceptron sederhana untuk mengklasifikasikan bunga Iris berdasarkan panjang dan lebar kelopak. [cite_start]Kita menggunakan Scikit-Learn karena ia menyediakan implementasi TLU tunggal yang efisien[cite: 201, 202]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Memuat dataset iris\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # Mengambil petal length dan petal width\n",
    "y = (iris.target == 0).astype(int)  # Apakah ini Iris Setosa? (Binary Classification)\n",
    "\n",
    "# Melatih model Perceptron\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "# Membuat prediksi\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(f\"Prediksi untuk input [2, 0.5]: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27302bff",
   "metadata": {},
   "source": [
    "### **Interpretasi Hasil**\n",
    "[cite_start]Model Perceptron di atas menemukan batas keputusan linear yang memisahkan kelas Iris Setosa dari kelas lainnya[cite: 198]. [cite_start]Namun, perlu dicatat bahwa Perceptron tidak memberikan probabilitas kelas, melainkan hanya prediksi berdasarkan ambang batas keras (*hard threshold*)[cite: 217, 218]. \n",
    "\n",
    "[cite_start]Salah satu kelemahan besar Perceptron adalah ketidakmampuannya menyelesaikan masalah yang tidak terpisahkan secara linear, seperti masalah *Exclusive OR* (XOR)[cite: 219]. [cite_start]Kelemahan ini dapat diatasi dengan menumpuk beberapa lapisan Perceptron, yang disebut **Multilayer Perceptron (MLP)**[cite: 221, 222]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533319fc",
   "metadata": {},
   "source": [
    "## **3. Multilayer Perceptron (MLP) dan Backpropagation**\n",
    "\n",
    "[cite_start]MLP terdiri dari satu lapisan input, satu atau lebih **lapisan tersembunyi (hidden layers)**, dan satu lapisan output[cite: 242]. [cite_start]Struktur ini memungkinkan model mempelajari pola yang jauh lebih kompleks[cite: 262, 263].\n",
    "\n",
    "[cite_start]Kunci kesuksesan MLP adalah algoritma **Backpropagation**[cite: 270]. Secara intuitif, Backpropagation adalah cara model \"belajar dari kesalahan\":\n",
    "1.  [cite_start]**Forward Pass:** Model membuat prediksi dan menyimpan hasil antara[cite: 284].\n",
    "2.  [cite_start]**Loss Measurement:** Membandingkan prediksi dengan target asli untuk melihat seberapa besar kesalahannya[cite: 285].\n",
    "3.  [cite_start]**Backward Pass:** Mengalirkan gradien kesalahan dari output kembali ke input menggunakan aturan rantai (chain rule) untuk melihat seberapa besar kontribusi setiap bobot terhadap kesalahan[cite: 287, 292].\n",
    "4.  [cite_start]**Gradient Descent Step:** Mengubah bobot sedikit demi sedikit untuk mengurangi kesalahan tersebut[cite: 293].\n",
    "\n",
    "### **Fungsi Aktivasi**\n",
    "[cite_start]Agar Backpropagation bekerja, kita harus mengganti fungsi tangga (*step function*) yang kaku dengan fungsi yang memiliki turunan yang jelas[cite: 299, 300]. Beberapa yang populer adalah:\n",
    "* [cite_start]**Sigmoid:** Menghasilkan output 0 hingga 1[cite: 299].\n",
    "* **ReLU (Rectified Linear Unit):** Menghasilkan output linear jika positif, dan nol jika negatif. [cite_start]Ini adalah pilihan default karena cepat dihitung[cite: 307, 308].\n",
    "* [cite_start]**Tanh:** Menghasilkan output -1 hingga 1, yang membantu memusatkan data di sekitar nol[cite: 303, 305]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81663a50",
   "metadata": {},
   "source": [
    "## **4. Menggunakan Keras untuk Klasifikasi Gambar**\n",
    "\n",
    "[cite_start]Keras adalah API tingkat tinggi yang membuat pembuatan dan pelatihan jaringan saraf menjadi sangat mudah[cite: 12, 408]. [cite_start]Kita akan menggunakan **Fashion MNIST**, sebuah dataset berisi 70.000 gambar item fesyen dalam skala abu-abu[cite: 465, 466].\n",
    "\n",
    "### **Reproduksi Kode: Membangun Model Sequential**\n",
    "\n",
    "[cite_start]Kode ini mendemonstrasikan **Sequential API**, cara termudah untuk membangun model dengan menumpuk lapisan secara berurutan[cite: 514, 515]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956182da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Memuat dataset Fashion MNIST\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# 2. Skalasi data (0-1) dan membagi set validasi\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# 3. Membangun Arsitektur MLP\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), # Mengubah 28x28 array menjadi 1D array\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # 10 neuron untuk 10 kelas fesyen\n",
    "])\n",
    "\n",
    "# 4. Kompilasi model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 5. Melatih model\n",
    "history = model.fit(X_train, y_train, epochs=10, \n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad39c25",
   "metadata": {},
   "source": [
    "### **Interpretasi Hasil**\n",
    "[cite_start]Selama pelatihan, kita dapat melihat metrik kerugian (*loss*) yang menurun dan akurasi yang meningkat[cite: 651]. [cite_start]Lapisan `Flatten` berfungsi untuk mengubah input 2D menjadi 1D agar bisa diproses oleh lapisan `Dense`[cite: 517]. [cite_start]Lapisan output menggunakan fungsi `softmax` untuk memastikan bahwa probabilitas semua kelas berjumlah 1, yang sangat penting dalam klasifikasi multiclass[cite: 377, 378]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042b91c",
   "metadata": {},
   "source": [
    "## **5. Membangun Model Kompleks: Functional API**\n",
    "\n",
    "[cite_start]Beberapa masalah memerlukan topologi non-linear, seperti model **Wide & Deep**[cite: 796]. [cite_start]Arsitektur ini memungkinkan model mempelajari pola mendalam melalui jalur lapisan tersembunyi, sekaligus aturan sederhana melalui jalur pintas langsung dari input ke output[cite: 797, 798].\n",
    "\n",
    "### **Reproduksi Kode: Functional API untuk Regresi**\n",
    "\n",
    "[cite_start]Mari kita gunakan dataset California housing untuk memprediksi harga rumah menggunakan Functional API[cite: 760, 812]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh struktur Wide & Deep menggunakan Functional API\n",
    "input_ = keras.layers.Input(shape=[8])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model_wide_deep = keras.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "model_wide_deep.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "print(\"Model Wide & Deep berhasil dibangun!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25610168",
   "metadata": {},
   "source": [
    "## **6. Menala Hyperparameter**\n",
    "\n",
    "[cite_start]Fleksibilitas jaringan saraf adalah salah satu kelebihan sekaligus tantangannya karena banyaknya hyperparameter yang harus diatur[cite: 1107]. Beberapa panduan umum:\n",
    "* **Jumlah Lapisan Tersembunyi:** Untuk banyak masalah, satu atau dua lapisan sudah cukup. [cite_start]Jaringan yang lebih dalam lebih efisien dalam parameter untuk memodelkan fungsi kompleks[cite: 1194, 1196].\n",
    "* [cite_start]**Jumlah Neuron:** Biasanya menggunakan jumlah yang sama di setiap lapisan tersembunyi, lalu gunakan *early stopping* untuk mencegah overfitting[cite: 1221, 1225].\n",
    "* **Learning Rate:** Hyperparameter paling penting. [cite_start]Nilai optimal biasanya sekitar setengah dari learning rate maksimum yang membuat model mulai divergen[cite: 1234, 1235]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdceeb80",
   "metadata": {},
   "source": [
    "## **7. Kesimpulan**\n",
    "\n",
    "[cite_start]Bab ini telah memberikan fondasi tentang bagaimana Jaringan Saraf Tiruan berevolusi dari model logika sederhana menjadi sistem Deep Learning yang canggih[cite: 1101]. Kita telah mempelajari:\n",
    "1. Perbedaan antara neuron biologis dan tiruan (TLU).\n",
    "2. Cara kerja Backpropagation untuk melatih Multilayer Perceptrons.\n",
    "3. Implementasi praktis menggunakan API Keras (Sequential dan Functional).\n",
    "4. Strategi dasar dalam pemilihan hyperparameter.\n",
    "\n",
    "[cite_start]Pengetahuan ini merupakan prasyarat penting sebelum kita mengeksplorasi teknik untuk melatih jaringan saraf yang sangat dalam serta arsitektur khusus lainnya seperti CNN dan RNN di bab-bab berikutnya[cite: 1269, 1271]."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
