{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732c2b02",
   "metadata": {},
   "source": [
    "# **Chapter 15: Processing Sequences Using RNNs and CNNs**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "Manusia memiliki kemampuan luar biasa untuk memprediksi masa depan berdasarkan urutan peristiwa masa lalu. Saat kita mendengar kalimat, kita bisa menebak kata berikutnya. Saat kita melihat bola dilempar, kita bisa mengantisipasi lintasannya.\n",
    "\n",
    "Dalam Machine Learning, data sekuensial (berurutan) seperti **deret waktu (time series)**, kalimat (teks), atau sinyal audio memerlukan penanganan khusus. Jaringan saraf biasa (Feedforward Neural Networks) tidak memiliki memori tentang apa yang terjadi sebelumnya. Di sinilah **Recurrent Neural Networks (RNNs)** dan **Convolutional Neural Networks (CNNs)** 1D berperan.\n",
    "\n",
    "Bab ini membahas:\n",
    "* Konsep dasar RNN dan bagaimana mereka \"mengingat\" informasi.\n",
    "* Melatih RNN menggunakan *Backpropagation Through Time*.\n",
    "* Memprediksi deret waktu (Time Series Forecasting).\n",
    "* Menangani sekuens panjang menggunakan **LSTM** dan **GRU**.\n",
    "* Menggunakan **1D CNN** untuk pemrosesan sekuens yang efisien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b7caa",
   "metadata": {},
   "source": [
    "## **2. Recurrent Neurons dan Layers**\n",
    "\n",
    "### **Intuisi Dasar**\n",
    "Bayangkan sebuah neuron yang tidak hanya menerima input dari data saat ini, tetapi juga menerima output dari dirinya sendiri pada langkah waktu sebelumnya. Ini adalah ide dasar **Recurrent Neuron**.\n",
    "\n",
    "Pada setiap langkah waktu $t$ (time step):\n",
    "* Neuron menerima input $x_{(t)}$.\n",
    "* Neuron menerima output dari langkah sebelumnya $y_{(t-1)}$.\n",
    "\n",
    "Ini menciptakan semacam memori jangka pendek. Jika kita membuka lipatan (*unroll*) proses ini terhadap waktu, RNN terlihat seperti jaringan saraf yang sangat dalam, di mana setiap layer memiliki bobot yang sama (shared weights).\n",
    "\n",
    "**Pentingnya dalam ML:**\n",
    "Kemampuan ini memungkinkan model untuk menangkap pola temporal dan dependensi antar waktu, yang sangat krusial untuk data seperti harga saham, cuaca, atau terjemahan bahasa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28011b8",
   "metadata": {},
   "source": [
    "## **3. Training RNN untuk Time Series Forecasting**\n",
    "\n",
    "Salah satu aplikasi paling umum dari RNN adalah memprediksi masa depan dalam deret waktu. Sebelum masuk ke model yang kompleks, mari kita buat data dummy (synthetic time series) untuk bereksperimen.\n",
    "\n",
    "Fungsi di bawah ini menghasilkan deret waktu yang terdiri dari dua gelombang sinus dengan frekuensi dan offset acak, ditambah sedikit noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # gelombang 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + gelombang 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "# Membuat dataset\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "print(\"Shape X_train:\", X_train.shape)\n",
    "print(\"Shape y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b535045",
   "metadata": {},
   "source": [
    "Mari kita visualisasikan salah satu sampel data deret waktu kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38755a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\"):\n",
    "    plt.plot(series, \".-\")\n",
    "    if y is not None:\n",
    "        plt.plot(n_steps, y, \"bx\", markersize=10)\n",
    "    if y_pred is not None:\n",
    "        plt.plot(n_steps, y_pred, \"ro\")\n",
    "    plt.grid(True)\n",
    "    if x_label:\n",
    "        plt.xlabel(x_label, fontsize=16)\n",
    "    if y_label:\n",
    "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(X_valid[0, :, 0], y_valid[0, 0])\n",
    "plt.title(\"Contoh Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0849c8",
   "metadata": {},
   "source": [
    "### **Baseline Metrics**\n",
    "Sebelum menggunakan RNN, kita harus memiliki standar pembanding (*baseline*).\n",
    "1.  **Naive Forecasting:** Prediksi nilai besok sama dengan nilai hari ini (nilai terakhir).\n",
    "2.  **Linear Regression:** Menggunakan Fully Connected Layer sederhana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29274390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Naive Forecasting\n",
    "y_pred = X_valid[:, -1]\n",
    "print(\"Naive MSE:\", np.mean(keras.losses.mean_squared_error(y_valid, y_pred)))\n",
    "\n",
    "# 2. Linear Prediction (Simple Dense Network)\n",
    "model_linear = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_linear.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_linear.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"Linear Model MSE:\", model_linear.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742ccd3",
   "metadata": {},
   "source": [
    "### **Implementasi Simple RNN**\n",
    "Sekarang mari kita gunakan `SimpleRNN` dari Keras. Layer ini hanya memiliki satu neuron recurrent.\n",
    "\n",
    "**Penting:**\n",
    "RNN mengharapkan input dalam bentuk 3D: `[batch_size, time_steps, dimensionality]`. Data kita sudah dalam format ini (batch, 50, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "model_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model_rnn.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"Simple RNN MSE:\", model_rnn.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad10c9f5",
   "metadata": {},
   "source": [
    "### **Deep RNNs**\n",
    "Satu neuron biasanya tidak cukup. Kita bisa menumpuk beberapa layer RNN.\n",
    "**Catatan Penting:** Jika kita menumpuk layer RNN, semua layer kecuali yang terakhir harus mengembalikan seluruh urutan output (`return_sequences=True`), bukan hanya output langkah terakhir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20), # Layer terakhir default return_sequences=False\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_deep_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_deep_rnn.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"Deep RNN MSE:\", model_deep_rnn.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df8a6f",
   "metadata": {},
   "source": [
    "## **4. Menangani Sekuens Panjang (Long Sequences)**\n",
    "\n",
    "Saat melatih RNN pada sekuens yang panjang, kita menghadapi masalah:\n",
    "1.  **Unstable Gradients:** Gradien bisa menghilang (*vanishing*) atau meledak (*exploding*), membuat pelatihan sulit.\n",
    "2.  **Memory Loss:** Informasi dari langkah awal cenderung terlupakan saat mencapai langkah akhir.\n",
    "\n",
    "Solusinya adalah menggunakan jenis sel yang lebih kompleks seperti **LSTM (Long Short-Term Memory)** dan **GRU (Gated Recurrent Unit)**.\n",
    "\n",
    "### **LSTM (Long Short-Term Memory)**\n",
    "\n",
    "LSTM memiliki \"jalur tol\" untuk memori jangka panjang yang memungkinkannya menyimpan informasi untuk waktu yang lama. Ia memiliki tiga gerbang (*gates*):\n",
    "* **Forget Gate:** Apa yang harus dilupakan dari memori jangka panjang.\n",
    "* **Input Gate:** Apa yang harus disimpan ke dalam memori.\n",
    "* **Output Gate:** Apa yang harus dikeluarkan pada langkah saat ini.\n",
    "\n",
    "### **GRU (Gated Recurrent Unit)**\n",
    "Versi sederhana dari LSTM yang menggabungkan beberapa gate, namun seringkali memiliki performa yang setara dengan komputasi yang lebih ringan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi LSTM\n",
    "model_lstm = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_lstm.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"LSTM MSE:\", model_lstm.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed51021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi GRU\n",
    "model_gru = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_gru.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"GRU MSE:\", model_gru.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624fe717",
   "metadata": {},
   "source": [
    "**Interpretasi Hasil:**\n",
    "Umumnya, LSTM dan GRU akan memberikan hasil MSE yang jauh lebih baik (lebih rendah) dibandingkan SimpleRNN, terutama jika pola dalam data memiliki ketergantungan jangka panjang. Mereka mampu \"mengingat\" tren atau siklus yang muncul jauh sebelumnya dalam urutan waktu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a06b4",
   "metadata": {},
   "source": [
    "## **5. Menggunakan 1D Convolutional Neural Networks**\n",
    "\n",
    "Meskipun CNN biasanya diasosiasikan dengan gambar (2D), CNN juga sangat efektif untuk data sekuensial (1D).\n",
    "\n",
    "**Bagaimana kerjanya?**\n",
    "Filter 1D meluncur di sepanjang urutan waktu (seperti jendela geser). Ini memungkinkan model untuk mempelajari pola lokal (misalnya, lonjakan singkat dalam harga saham) di mana pun posisinya dalam urutan waktu.\n",
    "\n",
    "**Keuntungan:**\n",
    "* Sangat efisien secara komputasi (bisa diparalelkan, tidak seperti RNN yang sekuensial).\n",
    "* Bisa dikombinasikan dengan RNN (misal: CNN untuk mengekstrak fitur, RNN untuk memori jangka panjang)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn1d = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
    "                        input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_cnn1d.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_cnn1d.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"1D CNN + GRU MSE:\", model_cnn1d.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b61250",
   "metadata": {},
   "source": [
    "### **WaveNet**\n",
    "Salah satu arsitektur CNN 1D yang terkenal adalah **WaveNet**. Ia menggunakan *dilated convolutions* (konvolusi yang melompati input dengan jarak tertentu) yang ditumpuk. Ini memungkinkan jaringan memiliki *receptive field* yang sangat besar (melihat sejarah yang sangat panjang) dengan jumlah parameter yang sedikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Sederhana WaveNet\n",
    "model_wavenet = keras.models.Sequential()\n",
    "model_wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "\n",
    "# Stack dilated convolutions\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "    model_wavenet.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
    "                                          activation=\"relu\", dilation_rate=rate))\n",
    "\n",
    "model_wavenet.add(keras.layers.Conv1D(filters=1, kernel_size=1))\n",
    "\n",
    "model_wavenet.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_wavenet.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "print(\"WaveNet MSE:\", model_wavenet.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e87ec",
   "metadata": {},
   "source": [
    "## **6. Kesimpulan**\n",
    "\n",
    "Dalam Chapter 15 ini, kita telah mempelajari:\n",
    "1.  **RNN** adalah alat utama untuk data sekuensial karena memiliki memori internal.\n",
    "2.  **Training RNN** bisa sulit karena masalah gradien yang tidak stabil, tetapi teknik seperti Layer Normalization membantu.\n",
    "3.  **LSTM dan GRU** mengatasi masalah memori jangka pendek dengan mekanisme *gating* yang cerdas, menjadi standar industri untuk banyak tugas sekuensial.\n",
    "4.  **1D CNN** menawarkan alternatif yang cepat dan efisien, seringkali mampu bersaing atau melengkapi RNN, terutama dengan arsitektur seperti WaveNet.\n",
    "\n",
    "Bab ini menutup pembahasan tentang *supervised learning* untuk data sekuensial. Di bab-bab selanjutnya (Part II buku), kita akan masuk ke topik yang lebih canggih seperti Natural Language Processing (NLP) dengan Transformer dan Generative Models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
