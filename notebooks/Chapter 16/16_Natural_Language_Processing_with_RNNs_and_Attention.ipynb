{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd2be6b",
   "metadata": {},
   "source": [
    "# **Chapter 16: Natural Language Processing with RNNs and Attention**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "Alan Turing pernah membayangkan sebuah tes untuk mengukur kecerdasan mesin, dan menariknya, ia memilih tugas linguistik (bahasa). Ini menunjukkan bahwa penguasaan bahasa adalah salah satu puncak kemampuan kognitif.\n",
    "\n",
    "Dalam Machine Learning, **Natural Language Processing (NLP)** adalah bidang yang berfokus pada interaksi antara komputer dan bahasa manusia.\n",
    "\n",
    "Bab ini membahas evolusi teknik NLP modern:\n",
    "* **Character-RNN (Char-RNN):** Memprediksi karakter berikutnya dalam kalimat untuk menghasilkan teks (misalnya, meniru gaya Shakespeare).\n",
    "* **Sentiment Analysis:** Mengklasifikasikan teks (misalnya, ulasan film) menjadi positif atau negatif.\n",
    "* **Neural Machine Translation (NMT):** Menerjemahkan bahasa menggunakan arsitektur **Encoder-Decoder**.\n",
    "* **Attention Mechanisms:** Mengatasi keterbatasan RNN tradisional dengan membiarkan model \"fokus\" pada bagian input tertentu.\n",
    "* **Transformer:** Arsitektur revolusioner yang sepenuhnya membuang RNN dan hanya mengandalkan mekanisme Attention (dasar dari model seperti BERT dan GPT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef226a",
   "metadata": {},
   "source": [
    "## **2. Generating Text dengan Character-RNN**\n",
    "\n",
    "### **Konsep Dasar**\n",
    "Ide dasarnya sederhana: berikan mesin serangkaian karakter, dan minta ia memprediksi karakter berikutnya. Jika kita melakukan ini berulang kali, kita bisa menghasilkan teks baru karakter demi karakter.\n",
    "\n",
    "Misalnya, jika inputnya \"Hell\", model harus memprediksi \"o\" untuk membentuk \"Hello\".\n",
    "\n",
    "### **Mempersiapkan Data**\n",
    "Langkah pertama adalah mengubah teks menjadi angka. Kita menggunakan *character-level tokenizer*. Setiap karakter unik dipetakan ke integer tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07055c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Mengunduh teks Shakespeare\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Tokenisasi pada level karakter\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])\n",
    "\n",
    "# Mengubah teks menjadi urutan angka\n",
    "max_id = len(tokenizer.word_index) # jumlah karakter unik\n",
    "dataset_size = tokenizer.document_count # total karakter\n",
    "encoded = np.array(tokenizer.texts_to_sequences([shakespeare_text]))[0]\n",
    "\n",
    "print(f\"Total karakter unik: {max_id}\")\n",
    "print(f\"Contoh encoded text: {encoded[:10]}\")\n",
    "print(f\"Mapping balik: {tokenizer.sequences_to_texts([encoded[:10]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58c524",
   "metadata": {},
   "source": [
    "### **Membangun dan Melatih Model Char-RNN**\n",
    "Untuk memprediksi karakter berikutnya, kita memotong teks menjadi jendela-jendela (*windows*) urutan input dan target.\n",
    "* **Input:** Karakter ke-1 sampai ke-n.\n",
    "* **Target:** Karakter ke-2 sampai ke-(n+1).\n",
    "\n",
    "Kita menggunakan layer **GRU (Gated Recurrent Unit)** karena lebih efisien daripada LSTM namun tetap mampu menangkap pola jangka panjang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dataset tf.data untuk pelatihan yang efisien\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input digeser 1 langkah ke depan\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.shuffle(10000).batch(32)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Membangun Model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2), # Dropout untuk regularisasi\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# model.fit(dataset, epochs=5) # Komentar: Pelatihan memakan waktu lama\n",
    "print(\"Model Char-RNN siap dilatih.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c1a3b",
   "metadata": {},
   "source": [
    "## **3. Sentiment Analysis**\n",
    "\n",
    "Selain menghasilkan teks, RNN sangat berguna untuk mengklasifikasikan urutan teks. Contoh klasiknya adalah **Sentiment Analysis** pada ulasan film IMDB (Positif vs Negatif).\n",
    "\n",
    "### **Preprocessing Data Teks**\n",
    "Berbeda dengan Char-RNN, di sini kita biasanya bekerja pada level **kata (word-level)**. Namun, alih-alih one-hot encoding (yang akan sangat besar dimensinya), kita menggunakan **Embeddings**.\n",
    "* **Word Embedding:** Representasi vektor padat (dense vector) di mana kata-kata dengan makna serupa memiliki representasi vektor yang berdekatan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset IMDB\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "\n",
    "# Word Index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# Menyesuaikan indeks (karena 3 indeks pertama dicadangkan)\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "print(\"Contoh ulasan ter-decode:\")\n",
    "print(\" \".join([id_to_word[id_] for id_ in X_train[0]]))\n",
    "\n",
    "# Membangun Model Klasifikasi\n",
    "# Kita menggunakan layer Embedding dan GRU\n",
    "model_sentiment = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=10000, output_dim=128), # Vocab size 10000\n",
    "    keras.layers.GRU(128, return_sequences=False),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\") # Output biner (Positif/Negatif)\n",
    "])\n",
    "\n",
    "model_sentiment.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(\"\\nArsitektur Model Sentimen:\")\n",
    "model_sentiment.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3448b2",
   "metadata": {},
   "source": [
    "## **4. Neural Machine Translation (NMT) dengan Encoder-Decoder**\n",
    "\n",
    "Untuk menerjemahkan kalimat (misal: Inggris ke Spanyol), panjang input dan output bisa berbeda. RNN biasa tidak bisa menangani ini secara langsung. Kita membutuhkan arsitektur **Encoder-Decoder**.\n",
    "\n",
    "### **Konsep Inti**\n",
    "1.  **Encoder:** RNN yang membaca kalimat input urut dan meringkasnya menjadi satu vektor status (*context vector*).\n",
    "2.  **Decoder:** RNN yang mengambil vektor status tersebut dan menghasilkan kalimat terjemahan langkah demi langkah.\n",
    "\n",
    "\n",
    "\n",
    "[Image of Encoder-Decoder Diagram]\n",
    "\n",
    "\n",
    "Ini sering disebut sebagai model **Seq2Seq**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4337ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa # Library tambahan sering digunakan untuk seq2seq\n",
    "\n",
    "# Parameter dummy\n",
    "vocab_size = 1000\n",
    "embed_size = 128\n",
    "units = 512\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "encoder_embeddings = keras.layers.Embedding(vocab_size, embed_size)(encoder_inputs)\n",
    "# return_state=True agar kita bisa mengambil hidden state terakhir\n",
    "encoder_outputs, state_h, state_c = keras.layers.LSTM(units, return_state=True)(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c] # Context Vector\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_embeddings = keras.layers.Embedding(vocab_size, embed_size)(decoder_inputs)\n",
    "# Initial state decoder diisi dengan state terakhir encoder\n",
    "decoder_lstm_layer = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm_layer(decoder_embeddings, initial_state=encoder_state)\n",
    "decoder_dense = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "model_nmt = keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model_nmt.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "print(\"Model NMT Sederhana:\")\n",
    "model_nmt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21b88e",
   "metadata": {},
   "source": [
    "**Penting:**\n",
    "Saat pelatihan, kita memberikan target yang benar ke decoder (teknik ini disebut *Teacher Forcing*). Namun, saat inferensi (penggunaan nyata), decoder harus menggunakan prediksi langkah sebelumnya sebagai input langkah berikutnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cda1f1",
   "metadata": {},
   "source": [
    "## **5. Attention Mechanisms**\n",
    "\n",
    "### **Masalah pada Encoder-Decoder Biasa**\n",
    "Memaksa Encoder meringkas seluruh kalimat menjadi *satu* vektor status itu sulit, terutama untuk kalimat panjang. Informasi awal sering terlupakan (Bottleneck problem).\n",
    "\n",
    "### **Solusi: Attention**\n",
    "Mekanisme Attention mengizinkan Decoder untuk \"melihat kembali\" (memperhatikan) seluruh urutan output Encoder pada setiap langkah waktu, bukan hanya status terakhirnya. Decoder belajar untuk memberikan **bobot** (fokus) yang berbeda pada kata-kata input yang berbeda saat menghasilkan kata output tertentu.\n",
    "\n",
    "* **Bahdanau Attention (Additive):** Menggunakan neural network kecil untuk menghitung skor alignment.\n",
    "* **Luong Attention (Multiplicative):** Menggunakan dot product untuk menghitung kesamaan.\n",
    "\n",
    "Di Keras, kita bisa menggunakan layer `tf.keras.layers.Attention` (Luong) atau `tf.keras.layers.AdditiveAttention` (Bahdanau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc9a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambahkan Attention ke NMT\n",
    "# Encoder (sekarang return_sequences=True karena Attention butuh semua output)\n",
    "encoder_outputs, state_h, state_c = keras.layers.LSTM(units, return_sequences=True, return_state=True)(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_lstm_layer = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm_layer(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "# Attention Layer\n",
    "# Menghubungkan output Decoder (query) dengan output Encoder (value/key)\n",
    "attention_layer = keras.layers.Attention()\n",
    "context_vector = attention_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Menggabungkan context vector dengan output decoder\n",
    "decoder_concat_input = keras.layers.Concatenate()([decoder_outputs, context_vector])\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation=\"softmax\"))(decoder_concat_input)\n",
    "\n",
    "model_attention = keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "print(\"Model NMT dengan Attention:\")\n",
    "model_attention.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab841d2b",
   "metadata": {},
   "source": [
    "## **6. The Transformer Architecture**\n",
    "\n",
    "Pada tahun 2017, paper *\"Attention Is All You Need\"* mengubah segalanya. Mereka memperkenalkan **Transformer**, arsitektur yang:\n",
    "1.  **Tidak menggunakan RNN sama sekali** (tidak ada recurrence).\n",
    "2.  Mengandalkan sepenuhnya pada **Self-Attention** dan Feed-Forward Networks.\n",
    "3.  Bisa diproses secara paralel (lebih cepat dilatih daripada RNN).\n",
    "\n",
    "### **Komponen Kunci:**\n",
    "* **Positional Encoding:** Karena tidak ada RNN, model tidak tahu urutan kata. Kita menyuntikkan informasi posisi secara matematis (menggunakan fungsi sinus/cosinus) ke dalam embedding input.\n",
    "* **Multi-Head Attention:** Model memperhatikan input dari beberapa \"perspektif\" representasi yang berbeda secara bersamaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Positional Encoding\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_steps = max_steps\n",
    "        self.max_dims = max_dims\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Membuat encoding sinus/cosinus\n",
    "        # (Implementasi detail rumus matematis sesuai paper)\n",
    "        if self.max_dims % 2 == 1: self.max_dims += 1 # genapkan\n",
    "        p, i = np.meshgrid(np.arange(self.max_steps), np.arange(self.max_dims // 2))\n",
    "        pos_emb = np.empty((1, self.max_steps, self.max_dims))\n",
    "        pos_emb[0, :, 0::2] = np.sin(p / 10000**(2 * i / self.max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / self.max_dims)).T\n",
    "        return inputs + pos_emb[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "# Visualisasi Positional Encoding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_steps = 200\n",
    "max_dims = 512\n",
    "pos_emb = PositionalEncoding(max_steps, max_dims)(np.zeros((1, max_steps, max_dims)))\n",
    "plt.pcolormesh(pos_emb[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.title(\"Visualisasi Positional Encoding Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d5878",
   "metadata": {},
   "source": [
    "**Dampak:**\n",
    "Arsitektur Transformer menjadi fondasi bagi model bahasa raksasa (LLM) modern seperti BERT, GPT-2, GPT-3, dan seterusnya. Kemampuannya menangani dependensi jarak jauh jauh lebih baik daripada LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d1284",
   "metadata": {},
   "source": [
    "## **7. Kesimpulan**\n",
    "\n",
    "Dalam Chapter 16 ini, kita telah menjelajahi evolusi pemrosesan bahasa alami:\n",
    "1.  **RNN & LSTM/GRU:** Dasar pemrosesan sekuens, mampu menangkap konteks temporal.\n",
    "2.  **Encoder-Decoder:** Memungkinkan pemetaan urutan-ke-urutan (seq2seq) untuk penerjemahan.\n",
    "3.  **Attention:** Terobosan yang mengatasi \"lupa\" pada kalimat panjang dengan membiarkan model fokus pada bagian relevan.\n",
    "4.  **Transformer:** Standar emas saat ini yang menggunakan *pure attention* untuk efisiensi dan performa tinggi.\n",
    "\n",
    "Pemahaman tentang mekanisme Attention adalah kunci untuk memahami perkembangan terkini dalam Deep Learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
