{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cccdaf56",
   "metadata": {},
   "source": [
    "# **Chapter 12: Custom Models and Training with TensorFlow**\n",
    "\n",
    "## **1. Pendahuluan**\n",
    "\n",
    "[cite_start]Hingga saat ini, kita telah menggunakan API tingkat tinggi TensorFlow, yaitu `tf.keras`, yang sangat memudahkan pembuatan berbagai arsitektur jaringan saraf. Faktanya, 95% kasus penggunaan yang akan Anda temui dapat diselesaikan hanya dengan `tf.keras`[cite: 1850].\n",
    "\n",
    "Namun, ada kalanya kita perlu menyelam lebih dalam ke API tingkat rendah TensorFlow. Hal ini diperlukan ketika kita membutuhkan kontrol ekstra untuk menulis:\n",
    "* Fungsi *loss* kustom.\n",
    "* Metrik, layer, atau model kustom.\n",
    "* Loop pelatihan (*training loop*) manual yang kompleks (misalnya untuk menerapkan transformasi gradien khusus).\n",
    "\n",
    "[cite_start]Bab ini akan membahas cara menggunakan API tingkat rendah Python TensorFlow untuk menangani kasus-kasus tersebut, serta bagaimana meningkatkan performa model kustom menggunakan fitur pembuatan grafik otomatis (*automatic graph generation*)[cite: 1853, 1854].\n",
    "\n",
    "\n",
    "Mari kita mulai dengan tur singkat mengenai dasar-dasar TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e1183",
   "metadata": {},
   "source": [
    "## **2. Menggunakan TensorFlow seperti NumPy**\n",
    "\n",
    "API TensorFlow berpusat pada **tensor**, yang mengalir dari satu operasi ke operasi lainnya. [cite_start]Tensor sangat mirip dengan `ndarray` pada NumPy, biasanya berupa array multidimensi, tetapi juga bisa menampung skalar[cite: 1964, 1965].\n",
    "\n",
    "### **Tensor dan Operasi**\n",
    "Kita dapat membuat tensor menggunakan `tf.constant()`. TensorFlow menyediakan berbagai operasi matematika dasar yang mirip dengan NumPy, seperti `tf.add`, `tf.multiply`, `tf.square`, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ccfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Membuat tensor matriks\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(\"Tensor t:\\n\", t)\n",
    "print(\"Shape:\", t.shape)\n",
    "print(\"Dtype:\", t.dtype)\n",
    "\n",
    "# Operasi Tensor\n",
    "# Penjumlahan dan pengkuadratan\n",
    "print(\"\\nt + 10:\\n\", t + 10)\n",
    "print(\"Square(t):\\n\", tf.square(t))\n",
    "\n",
    "# Perkalian matriks (@ operator)\n",
    "print(\"t @ t.T:\\n\", t @ tf.transpose(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc338d",
   "metadata": {},
   "source": [
    "### **Variabel**\n",
    "Tensor biasa bersifat *immutable* (tidak dapat diubah). [cite_start]Untuk parameter model (seperti bobot dan bias) yang perlu diperbarui selama *backpropagation*, kita membutuhkan **`tf.Variable`**[cite: 2066, 2069].\n",
    "\n",
    "Variabel dapat dimodifikasi *in-place* menggunakan metode `assign()`, `assign_add()`, atau `assign_sub()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "# Mengubah nilai variabel\n",
    "v.assign(2 * v)           # Mengalikan nilai dengan 2\n",
    "v[0, 1].assign(42)        # Mengubah elemen spesifik\n",
    "\n",
    "print(\"Variabel v yang telah dimodifikasi:\\n\", v.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d0ab7",
   "metadata": {},
   "source": [
    "## **3. Custom Loss Functions**\n",
    "\n",
    "Terkadang fungsi *loss* standar tidak cukup untuk kebutuhan spesifik kita. Misalnya, untuk data yang *noisy* (berisik), *Mean Squared Error* (MSE) mungkin terlalu sensitif terhadap outlier, sedangkan *Mean Absolute Error* (MAE) mungkin kurang presisi saat konvergensi. [cite_start]**Huber Loss** adalah solusi di antara keduanya[cite: 2116, 2117, 2118].\n",
    "\n",
    "Mari kita implementasikan Huber Loss secara manual. Fungsi ini harus menerima label asli (`y_true`) dan prediksi (`y_pred`), lalu mengembalikan nilai *loss* per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Penggunaan dalam model Keras\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=[8])])\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "print(\"Model berhasil dikompilasi dengan custom loss function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec39e05",
   "metadata": {},
   "source": [
    "### **Menyimpan Model dengan Komponen Kustom**\n",
    "Saat memuat model yang memiliki komponen kustom, kita perlu memetakan nama fungsi ke fungsi aslinya. [cite_start]Jika *loss function* memiliki *hyperparameter* (misalnya `threshold` pada Huber Loss), lebih baik mengimplementasikannya sebagai *subclass* dari `keras.losses.Loss` agar konfigurasinya ikut tersimpan[cite: 2162, 2164]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "        \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# Penggunaan kelas custom loss\n",
    "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a7cfe",
   "metadata": {},
   "source": [
    "## **4. Custom Metrics**\n",
    "\n",
    "Metrik digunakan untuk evaluasi dan harus mudah diinterpretasikan manusia. Tidak seperti *loss*, metrik tidak perlu *differentiable*.\n",
    "\n",
    "Untuk metrik sederhana, kita bisa menggunakan fungsi seperti *loss*. [cite_start]Namun, untuk metrik yang bersifat *streaming* (dihitung secara bertahap per *batch*, seperti **Precision**), kita perlu membuat *subclass* dari `keras.metrics.Metric`[cite: 2264, 2271].\n",
    "\n",
    "Objek *streaming metric* menjaga status internal (variabel) yang diperbarui setiap *batch* melalui metode `update_state()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        # Variabel status\n",
    "        self.huber_fn = lambda y_true, y_pred: HuberLoss(threshold).call(y_true, y_pred)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "        \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be34696",
   "metadata": {},
   "source": [
    "## **5. Custom Layers**\n",
    "\n",
    "Jika Anda membutuhkan layer yang tidak tersedia di Keras, Anda bisa membuatnya sendiri.\n",
    "* [cite_start]**Layer tanpa bobot (Stateless):** Gunakan `keras.layers.Lambda`[cite: 2311].\n",
    "* [cite_start]**Layer dengan bobot (Stateful):** Buat *subclass* dari `keras.layers.Layer`[cite: 2317].\n",
    "\n",
    "Layer kustom harus mengimplementasikan:\n",
    "1.  [cite_start]`build()`: Membuat bobot layer (dipanggil saat *shape* input diketahui)[cite: 2343].\n",
    "2.  [cite_start]`call()`: Melakukan operasi perhitungan (forward pass)[cite: 2353].\n",
    "3.  [cite_start]`compute_output_shape()`: Menentukan bentuk output[cite: 2354]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        # Membuat bobot kernel dan bias\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        # Operasi: X * W + b\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f0f7d",
   "metadata": {},
   "source": [
    "## **6. Custom Models**\n",
    "\n",
    "Untuk arsitektur yang kompleks (misalnya dengan *skip connections* atau loop), kita bisa membuat *subclass* dari `keras.Model`. [cite_start]Ini mirip dengan custom layer, tetapi dengan fungsi tambahan seperti `compile()`, `fit()`, dan `save()`[cite: 2385, 2386].\n",
    "\n",
    "Contoh implementasi model dengan **Residual Block** (seperti pada ResNet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f26a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z # Skip connection: menambahkan input asli ke output\n",
    "\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"elu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff9c2e",
   "metadata": {},
   "source": [
    "### **Losses Berdasarkan Internal Model**\n",
    "Terkadang kita ingin menghitung *loss* berdasarkan bagian internal model, bukan hanya outputnya. Contohnya adalah **Reconstruction Loss** pada *autoencoder* untuk regularisasi. [cite_start]Kita bisa menggunakan metode `add_loss()` di dalam metode `call()`[cite: 2445, 2448]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfbd23",
   "metadata": {},
   "source": [
    "## **7. Menghitung Gradien dengan Autodiff**\n",
    "\n",
    "Untuk memahami cara kerja *training loop* kustom, kita perlu memahami **Automatic Differentiation (Autodiff)**. [cite_start]TensorFlow menggunakan `tf.GradientTape` untuk merekam operasi agar gradien dapat dihitung secara otomatis[cite: 2516, 2519].\n",
    "\n",
    "Tape akan merekam operasi yang melibatkan variabel, lalu kita bisa memanggil `tape.gradient()` untuk mendapatkan turunannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39267fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1**2 + 2 * w1 * w2\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(\"Gradien:\", gradients) \n",
    "# Turunan thd w1: 6*w1 + 2*w2 = 30 + 6 = 36\n",
    "# Turunan thd w2: 2*w1 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f20af2",
   "metadata": {},
   "source": [
    "## **8. Custom Training Loops**\n",
    "\n",
    "Meskipun metode `fit()` sangat kuat, ada kalanya kita membutuhkan kendali penuh atas proses pelatihan (misalnya menggunakan dua optimizer berbeda). Kita bisa menulis loop pelatihan sendiri.\n",
    "\n",
    "Langkah-langkah utamanya:\n",
    "1.  Iterasi dataset per *batch*.\n",
    "2.  Gunakan `tf.GradientTape` untuk menghitung *loss*.\n",
    "3.  Hitung gradien menggunakan tape.\n",
    "4.  [cite_start]Terapkan gradien ke optimizer menggunakan `apply_gradients()`[cite: 2635, 2639, 2653]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh kerangka Custom Training Loop\n",
    "# (Pastikan model dan optimizer sudah didefinisikan sebelumnya)\n",
    "\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "#     for step, (X_batch, y_batch) in enumerate(train_set):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = model(X_batch, training=True)\n",
    "#             loss = loss_fn(y_batch, y_pred)\n",
    "#         \n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb579bb",
   "metadata": {},
   "source": [
    "## **9. TensorFlow Functions dan Graphs**\n",
    "\n",
    "TensorFlow 2.0 menggunakan mode *eager execution* secara default yang mudah di-debug tetapi kurang optimal. [cite_start]Untuk meningkatkan performa, kita bisa mengubah fungsi Python menjadi **TensorFlow Function** (grafik komputasi) menggunakan dekorator `@tf.function`[cite: 2684, 2696].\n",
    "\n",
    "[cite_start]Fitur **AutoGraph** akan menganalisis kode Python (termasuk loop `for`, `if`, dll.) dan mengubahnya menjadi operasi grafik TensorFlow yang efisien[cite: 2722]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc81c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "print(\"TF Function result:\", cube(tf.constant(2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59bea9f",
   "metadata": {},
   "source": [
    "## **Kesimpulan**\n",
    "\n",
    "Bab ini telah membekali Anda dengan kemampuan untuk menyesuaikan hampir setiap aspek dari TensorFlow:\n",
    "* Menggunakan operasi Tensor tingkat rendah.\n",
    "* Membuat *Loss*, *Metric*, *Layer*, dan *Model* kustom.\n",
    "* Memahami *Automatic Differentiation* dengan `GradientTape`.\n",
    "* Menulis *Training Loop* manual untuk fleksibilitas maksimal.\n",
    "* Mengoptimalkan performa dengan `@tf.function` dan grafik.\n",
    "\n",
    "Kemampuan ini sangat penting ketika Anda ingin mengimplementasikan ide-ide penelitian terbaru yang belum tersedia di pustaka standar Keras."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
